<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-blue?logo=python">
  <img src="https://img.shields.io/badge/Flask-3.0-black?logo=flask">
  <img src="https://img.shields.io/badge/OpenAI%20CLIP-ViT%2FB32-green?logo=openai">
  <img src="https://img.shields.io/badge/Status-Active-success?style=flat">
</p>

<h1 align="center">🧠 AI-Powered Multimodal Gallery Search</h1>

<p align="center">
  Search your <b>images and videos using natural language</b>.<br>
  Built for the <b>Multimodal AI Hackathon</b> using OpenAI CLIP + Flask.
</p>

---

## 🚀 Overview
**AI-Powered Multimodal Gallery Search** is a project developed for a **Multimodal AI Hackathon**.  
It allows users to **search their gallery of images and videos using natural text** — for example:  
> _"dog playing in a park"_  

and it instantly returns all matching media, even if filenames don’t match.

This project connects **text, image, and video understanding** into a single intelligent system.

---

## 🌐 Demo (Local Setup)
You can run this project locally using Flask.  
It includes:
- **Frontend:** HTML, CSS, JavaScript  
- **Backend:** Python (Flask + OpenAI CLIP)  
- **Model:** `openai/clip-vit-base-patch32`

---

## 📂 Project Structure
├── app.py # Flask backend

├── static/

│ ├── css/

│  └── style.css # Frontend styling

│ ├── js/

│  └── script.js # Frontend logic

│ ├── uploads/ # Uploaded media stored here

├── templates/

  └── index.html # Main web interface

