<p align="center"> <img src="https://img.shields.io/badge/Python-3.10+-blue?logo=python"> <img src="https://img.shields.io/badge/Flask-3.0-black?logo=flask"> <img src="https://img.shields.io/badge/OpenAI%20CLIP-ViT%2FB32-green?logo=openai"> <img src="https://img.shields.io/badge/Status-Active-success?style=flat"> </p> <h1 align="center">ğŸ§  AI-Powered Multimodal Gallery Search</h1> <p align="center"> Search your <b>images and videos using natural language</b>.<br> Built for the <b>Multimodal AI Hackathon</b> using OpenAI CLIP + Flask. </p>
ğŸš€ Overview

AI-Powered Multimodal Gallery Search is a project developed for a Multimodal AI Hackathon.
It allows users to search their gallery of images and videos using natural text â€” for example:
"dog playing in a park"
and it instantly returns all matching media, even if filenames donâ€™t match.

This project connects text, image, and video understanding into a single intelligent system.
ğŸŒ Demo (Local Setup)

You can run this project locally using Flask.
It includes:

Frontend: HTML, CSS, JavaScript

Backend: Python (Flask + OpenAI CLIP)

Model: openai/clip-vit-base-patch32
