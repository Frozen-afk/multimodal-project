<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-blue?logo=python">
  <img src="https://img.shields.io/badge/Flask-3.0-black?logo=flask">
  <img src="https://img.shields.io/badge/OpenAI%20CLIP-ViT%2FB32-green?logo=openai">
  <img src="https://img.shields.io/badge/Status-Active-success?style=flat">
</p>

<h1 align="center">ğŸ§  AI-Powered Multimodal Gallery Search</h1>

<p align="center">
  Search your <b>images and videos using natural language</b>.<br>
  Built for the <b>Multimodal AI Hackathon</b> using OpenAI CLIP + Flask.
</p>

---

## ğŸš€ Overview
**AI-Powered Multimodal Gallery Search** is a project developed for a **Multimodal AI Hackathon**.  
It allows users to **search their gallery of images and videos using natural text** â€” for example:  
> _"dog playing in a park"_  

and it instantly returns all matching media, even if filenames donâ€™t match.

This project connects **text, image, and video understanding** into a single intelligent system.

---

## ğŸŒ Demo (Local Setup)
You can run this project locally using Flask.  
It includes:
- **Frontend:** HTML, CSS, JavaScript  
- **Backend:** Python (Flask + OpenAI CLIP)  
- **Model:** `openai/clip-vit-base-patch32`

---

## ğŸ“‚ Project Structure
â”œâ”€â”€ app.py # Flask backend

â”œâ”€â”€ static/

â”‚ â”œâ”€â”€ css/

â”‚  â””â”€â”€ style.css # Frontend styling

â”‚ â”œâ”€â”€ js/

â”‚  â””â”€â”€ script.js # Frontend logic

â”‚ â”œâ”€â”€ uploads/ # Uploaded media stored here

â”œâ”€â”€ templates/

  â””â”€â”€ index.html # Main web interface

